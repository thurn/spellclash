// Copyright Â© Dungeon of the Diamond Queen 2024-present
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//   https://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

//! An implementation of Monte Carlo Tree Search.
//!
//! This implementation is based on the pseudocode given in "A Survey of Monte
//! Carlo Tree Search Methods" by Browne et al. in IEEE Transactions on
//! Computational Intelligence and AI in Games, Vol. 4, No. 1, March 2012.

use std::collections::HashSet;
use std::marker::PhantomData;
use std::time::Instant;

use petgraph::prelude::{EdgeRef, NodeIndex};
use petgraph::{Direction, Graph};
use rand::prelude::IteratorRandom;
use tracing::debug;

use crate::core::agent::AgentConfig;
use crate::core::game_state_node::{GameStateNode, GameStatus};
use crate::core::selection_algorithm::SelectionAlgorithm;
use crate::core::state_evaluator::StateEvaluator;
use crate::monte_carlo::child_score::{ChildScoreAlgorithm, SelectionMode};

/// Plays out a game using random moves until a terminal state is reached, then
/// evaluates the result using the provided state evaluator.
///
/// Pseudocode:
/// ```text
/// ğŸğ®ğ§ğœğ­ğ¢ğ¨ğ§ DEFAULTPOLICY(s)
///   ğ°ğ¡ğ¢ğ¥ğ s is non-terminal ğğ¨
///     choose ğ’‚ âˆˆ A(s) uniformly at random
///     s â† f(s,ğ’‚)
///   ğ«ğğ­ğ®ğ«ğ§ reward for state s
/// ```
pub struct RandomPlayoutEvaluator<TState: GameStateNode + Send, TEvaluator: StateEvaluator<TState>>
{
    pub evaluator: TEvaluator,
    pub phantom_data: PhantomData<TState>,
}

impl<TState: GameStateNode + Send, TEvaluator: StateEvaluator<TState>> StateEvaluator<TState>
    for RandomPlayoutEvaluator<TState, TEvaluator>
{
    fn evaluate(&self, input: &TState, player: TState::PlayerName) -> i32 {
        let mut game = input.make_copy();
        loop {
            match game.status() {
                GameStatus::Completed { .. } => {
                    return self.evaluator.evaluate(&game, player);
                }
                GameStatus::InProgress { current_turn } => {
                    let action = game
                        .legal_actions(current_turn)
                        .choose(&mut rand::thread_rng())
                        .expect("No actions found");
                    game.execute_action(current_turn, action);
                }
            }
        }
    }
}

#[derive(Debug, Clone)]
struct SearchNode<TState: GameStateNode> {
    /// Player who acted to create this node
    pub player: TState::PlayerName,
    /// Q(v): Total reward of all playouts that passed through this state
    pub total_reward: f64,
    /// N(v): Visit count for this node
    pub visit_count: u32,
}

struct SearchEdge<TState: GameStateNode> {
    pub action: TState::Action,
}

type SearchGraph<TState> = Graph<SearchNode<TState>, SearchEdge<TState>>;

/// Monte Carlo search algorithm.
///
/// Monte carlo tree search operates over a tree of game state nodes
/// connected by game actions. The search follows these three steps
/// repeatedly:
///
/// 1) **Tree Policy:** Find a node in the tree which has not previously
/// been explored. The UCT algorithm is one mathematical heuristic
/// for how to prioritize nodes to explore.
///
/// 2) **Default Policy:** Score this node to determine its reward value
/// (âˆ†), typically by playing random moves until the game terminates.
///
/// 3) **Backpropagation:** Walk back up the tree, adding the resulting
/// reward value to each parent node.
///
/// Pseudocode:
/// ```text
/// ğŸğ®ğ§ğœğ­ğ¢ğ¨ğ§ UCTSEARCH(sâ‚€)
///   create root node vâ‚€ with state sâ‚€
///   ğ°ğ¡ğ¢ğ¥ğ within computational budget ğğ¨
///     vâ‚ â† TREEPOLICY(vâ‚€)
///     âˆ† â† DEFAULTPOLICY(s(vâ‚))
///     BACKUP(vâ‚, âˆ†)
///   ğ«ğğ­ğ®ğ«ğ§ ğ’‚(BESTCHILD(vâ‚€, 0))
/// ```
pub struct MonteCarloAlgorithm<TScoreAlgorithm: ChildScoreAlgorithm> {
    pub child_score_algorithm: TScoreAlgorithm,
    pub max_iterations: Option<u32>,
}

impl<TScoreAlgorithm: ChildScoreAlgorithm> SelectionAlgorithm
    for MonteCarloAlgorithm<TScoreAlgorithm>
{
    fn pick_action<TStateNode, TEvaluator>(
        &self,
        config: AgentConfig,
        node: &TStateNode,
        evaluator: &TEvaluator,
        player: TStateNode::PlayerName,
    ) -> TStateNode::Action
    where
        TStateNode: GameStateNode,
        TEvaluator: StateEvaluator<TStateNode>,
    {
        self.run_search(
            |i| {
                (i % 100 == 0 && config.deadline < Instant::now())
                    || self.max_iterations.map_or(false, |max| i > max)
            },
            node,
            evaluator,
            player,
        )
    }
}

impl<TScoreAlgorithm: ChildScoreAlgorithm> MonteCarloAlgorithm<TScoreAlgorithm> {
    pub fn run_search<TStateNode: GameStateNode, TEvaluator: StateEvaluator<TStateNode>>(
        &self,
        should_halt: impl Fn(u32) -> bool,
        node: &TStateNode,
        evaluator: &TEvaluator,
        player: TStateNode::PlayerName,
    ) -> TStateNode::Action {
        let mut graph = SearchGraph::new();
        let root = graph.add_node(SearchNode { total_reward: 0.0, visit_count: 1, player });
        let mut i = 0;
        while !should_halt(i) {
            let mut game = node.make_copy();
            let node = self.tree_policy(&mut graph, &mut game, root);
            let reward = f64::from(evaluator.evaluate(&game, player));
            Self::backup(&mut graph, player, node, reward);
            i += 1;
        }

        let (action, _) = self.best_child(
            &graph,
            root,
            node.legal_actions(player).collect(),
            SelectionMode::Best,
        );

        self.log_results(node, player, &graph, root);
        action
    }

    fn log_results<TStateNode: GameStateNode>(
        &self,
        node: &TStateNode,
        player: TStateNode::PlayerName,
        graph: &SearchGraph<TStateNode>,
        root: NodeIndex,
    ) {
        let parent_visits = graph[root].visit_count;
        let mut edges = graph
            .edges(root)
            .filter(|edge| node.legal_actions(player).any(|a| a == edge.weight().action))
            .map(|edge| {
                let child = &graph[edge.target()];
                (
                    edge,
                    self.child_score_algorithm.score(
                        f64::from(parent_visits),
                        f64::from(child.visit_count),
                        child.total_reward,
                        SelectionMode::Best,
                    ),
                )
            })
            .collect::<Vec<_>>();
        edges.sort_by(|(_, a), (_, b)| a.partial_cmp(b).unwrap());
        edges.reverse();

        for (child, weight) in edges.iter().map(|(edge, weight)| (edge.weight().action, *weight)) {
            debug!("Action: {:?} at {:?}", weight, child);
        }
    }

    /// Returns a descendant node to examine next for the provided parent node,
    /// either:
    ///  * A node which has not yet been explored
    ///  * The best terminal node descendant, if all nodes have been explored.
    ///
    /// If possible actions are available from this node which have not yet been
    /// explored, selects an action and applies it, returning the result as a
    /// new child. Otherwise, selects the best child to explore based on
    /// visit counts and known rewards, using the [best_child] algorithm,
    /// and then repeats this process recursively until an unseen node is
    /// found (or the best child is terminal).
    ///
    /// Mutates the provided [GameState] to represent the game state at the
    /// returned node.
    ///
    /// Cáµ– is the exploration constant, Cáµ– = 1/âˆš2 was suggested by Kocsis and
    /// SzepesvaÌri as a good choice.
    ///
    /// Pseudocode:
    /// ```text
    /// ğŸğ®ğ§ğœğ­ğ¢ğ¨ğ§ TREEPOLICY(v)
    ///   ğ°ğ¡ğ¢ğ¥ğ v is nonterminal ğğ¨
    ///     ğ¢ğŸ v not fully expanded ğ­ğ¡ğğ§
    ///       ğ«ğğ­ğ®ğ«ğ§ EXPAND(v)
    ///     ğğ¥ğ¬ğ
    ///       v â† BESTCHILD(v, Cáµ–)
    ///   ğ«ğğ­ğ®ğ«ğ§ v
    /// ```
    fn tree_policy<TState: GameStateNode>(
        &self,
        graph: &mut SearchGraph<TState>,
        game: &mut TState,
        mut node: NodeIndex,
    ) -> NodeIndex {
        while let GameStatus::InProgress { current_turn } = game.status() {
            let actions = game.legal_actions(current_turn).collect::<HashSet<_>>();
            let explored = graph.edges(node).map(|e| e.weight().action).collect::<HashSet<_>>();
            if let Some(action) = actions.iter().find(|a| !explored.contains(a)) {
                // An action exists which has not yet been tried
                return Self::expand(graph, game, current_turn, node, *action);
            } else {
                // All actions have been tried, recursively search the best candidate
                let (action, best) =
                    self.best_child(graph, node, actions, SelectionMode::Exploration);
                game.execute_action(current_turn, action);
                node = best;
            }
        }
        node
    }

    /// Generates a new tree node by applying the next untried action from the
    /// provided input node. Mutates the provided [GameState] to apply the
    /// provided game action.
    ///
    /// Pseudocode:
    /// ```text
    /// ğŸğ®ğ§ğœğ­ğ¢ğ¨ğ§ EXPAND(v)
    ///   choose ğ’‚ âˆˆ untried actions from A(s(v))
    ///   add a new child vâ€² to v
    ///     with s(vâ€²) = f(s(v), ğ’‚)
    ///     and ğ’‚(vâ€²) = ğ’‚
    ///   ğ«ğğ­ğ®ğ«ğ§ vâ€²
    /// ```
    fn expand<TState: GameStateNode>(
        graph: &mut SearchGraph<TState>,
        game: &mut TState,
        player: TState::PlayerName,
        source: NodeIndex,
        action: TState::Action,
    ) -> NodeIndex {
        game.execute_action(player, action);
        let target = graph.add_node(SearchNode { player, total_reward: 0.0, visit_count: 0 });
        graph.add_edge(source, target, SearchEdge { action });
        target
    }

    /// Picks the most promising child node to explore, returning its associated
    /// action and node identifier.
    ///
    /// This implementation is using the UCT1 algorithm, a standard approach for
    /// selecting children and solution to the 'multi-armed bandit' problem.
    ///
    /// Pseudocode:
    /// ```text
    /// ğŸğ®ğ§ğœğ­ğ¢ğ¨ğ§ BESTCHILD(v,c)
    ///   ğ«ğğ­ğ®ğ«ğ§ argmax(
    ///     vâ€² âˆˆ children of v:
    ///     Q(vâ€²) / N(vâ€²) +
    ///     c * âˆš [ 2 * ln(N(v)) / N(vâ€²) ]
    ///   )
    /// ```
    fn best_child<TState: GameStateNode>(
        &self,
        graph: &SearchGraph<TState>,
        node: NodeIndex,
        legal: HashSet<TState::Action>,
        selection_mode: SelectionMode,
    ) -> (TState::Action, NodeIndex) {
        let parent_visits = graph[node].visit_count;
        let (edge, _) = graph
            .edges(node)
            // We re-check action legality here because the set of legal actions can change between
            // visits, e.g. if different cards are drawn
            .filter(|edge| legal.contains(&edge.weight().action))
            .map(|edge| {
                let child = &graph[edge.target()];
                // This can technically panic when invoked from root with a very small
                // simulation count, so don't do that :)
                assert_ne!(child.visit_count, 0);
                (
                    edge,
                    self.child_score_algorithm.score(
                        f64::from(parent_visits),
                        f64::from(child.visit_count),
                        child.total_reward,
                        selection_mode,
                    ),
                )
            })
            .max_by(|(_, a), (_, b)| a.partial_cmp(b).unwrap())
            .expect("No children found");
        (edge.weight().action, edge.target())
    }

    /// Once a playout is completed, the backpropagation step walks back up the
    /// hierarchy of parent nodes, adding the resulting reward value to each
    /// one.
    ///
    /// Pseudocode:
    /// ```text
    /// ğŸğ®ğ§ğœğ­ğ¢ğ¨ğ§ BACKUP(v,âˆ†)
    ///   ğ°ğ¡ğ¢ğ¥ğ v is not null ğğ¨
    ///     N(v) â† N(v) + 1
    ///     Q(v) â† Q(v) + âˆ†(v, p)
    ///     v â† parent of v
    /// ```
    fn backup<TState: GameStateNode>(
        graph: &mut SearchGraph<TState>,
        maximizing_player: TState::PlayerName,
        mut node: NodeIndex,
        reward: f64,
    ) {
        loop {
            let weight = graph.node_weight_mut(node).expect("Node not found");
            weight.visit_count += 1;
            weight.total_reward +=
                if weight.player == maximizing_player { reward } else { -reward };

            node = match graph.neighbors_directed(node, Direction::Incoming).next() {
                Some(n) => n,
                _ => return,
            };
        }
    }
}
